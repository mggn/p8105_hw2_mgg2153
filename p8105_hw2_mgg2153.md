p8105\_hw2\_mgg2153
================
mggn
9/25/2020

## Hello and welcome to the second homework

The first order of business is to load the tidyverse, which I did in a
code chunk but I am hiding the chunk from showing when the file it knit
:-) I also loaded the readxl package, as it is needed for the Mr. Trash
Wheel dataset

### Problem 1: Mr. Trash Wheel (done in class)

We will read in the precipitation data for 2017 and 2018 Note: file name
in lecture is different than the file name used below

``` r
trashwheel_df = 
    read_xlsx("./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
        sheet = "Mr. Trash Wheel",
        range = cell_cols("A:N")) %>% 
    janitor::clean_names() %>% 
    drop_na(dumpster) %>% 
    mutate(
        sports_balls = round(sports_balls),
        sports_balls = as.integer(sports_balls)
    )

precip_2018 = 
    read_excel( "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
        sheet = "2018 Precipitation",
        skip = 1
    ) %>% 
    janitor::clean_names() %>% 
    drop_na(month) %>% 
    mutate(year = 2018) %>% 
    relocate(year)

precip_2017 = 
    read_excel(
        "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
        sheet = "2017 Precipitation",
        skip = 1
    ) %>% 
    janitor::clean_names() %>% 
    drop_na(month) %>% 
    mutate(year = 2017) %>% 
    relocate(year)
```

Next, the two dataframes will be combined by the variable month. This
will be possible through the creation of dataframe month\_df, and then
performing a left join using the variable month

``` r
month_df = 
    tibble(
        month = 1:12,
        month_name = month.name
    )

precip_df = 
    bind_rows(precip_2018, precip_2017)

precip_df =
    left_join(precip_df, month_df, by = "month")
```

#### About the Mr. Trashwheel dataset

Mr. Trashwheel is a “water vessel that removes trash from the Inner
Harbor in Baltimore, MD.” Mr. Trashwheel has eyes\!

There are a total of **344** observations and **14** variables, which
are **dumpster, month, year, date, weight\_tons, volume\_cubic\_yards,
plastic\_bottles, polystyrene, cigarette\_butts, glass\_bottles,
grocery\_bags, chip\_bags, sports\_balls, homes\_powered**

The median number of sports balls found in a dumpster in 2017 was **8**.

The total precipitation in 2018 was **70.33** inches.

### Problem 2: NYC transit data

First, I want to (1) read in the data (2) clean the variable names to
follow good naming conventions (3) keep only the variables I want (4)
recode entry to TRUE/FALSE instead of YES/NO

``` r
transit_df = read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names() %>%
  select(line, station_name, station_latitude, station_longitude, 
         route1:route11, entrance_type, entry, vending, ada) %>%
  mutate(
    entry = recode(entry, "YES" = "TRUE", "NO" = "FALSE")
  )
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_character(),
    ##   `Station Latitude` = col_double(),
    ##   `Station Longitude` = col_double(),
    ##   Route8 = col_double(),
    ##   Route9 = col_double(),
    ##   Route10 = col_double(),
    ##   Route11 = col_double(),
    ##   ADA = col_logical(),
    ##   `Free Crossover` = col_logical(),
    ##   `Entrance Latitude` = col_double(),
    ##   `Entrance Longitude` = col_double()
    ## )

    ## See spec(...) for full column specifications.

#### About the NYC transit data

This data set is about New York City’s subway system, particularly the
subway exits and entrances. It is important to note that the dataset
being described below is the dataset that has been cleaned with the code
above.

The dataset has **1868** observations and **19** variables, which are:
**line, station\_name, station\_latitude, station\_longitude, route1,
route2, route3, route4, route5, route6, route7, route8, route9, route10,
route11, entrance\_type, entry, vending, ada**

There are **465** distinct stations.

There are **468** stations that are ADA compliant.

And we can see that **69** stations without vending allow entry, meaning
the proportion is **0.0393611**

After running these commands, Jeff asks us:

#### are these data tidy?

To which I respond no\! And Hadley Wickham would say “hell no” One
example of the lack of tidiness is that the routes, for example, are
treated as their own separate variables (ie, each route has its own
column) when, to be normalized, the routes should be collapsed into one
variable. We can normalize/ “tidy” the data by using pivot\_longer:

``` r
transit_df_tidy =
  transit_df %>%
  mutate(
    route8 = as.character(route8),
    route9 = as.character(route9),
    route10 = as.character(route10),
    route11 = as.character(route11)
    ) %>%
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route_name"
  )
```

With this “tidier” dataset, we can see that there are **12** distinct A
stations, and **107** A stations that are ADA compliant.

### Problem 3: FiveThirtyEight data

To start I will: (1) load in the pols dataset (2) clean names (3)
separate the date into three different variables, month, day, year (4)
format date into text form using month.abb (5) create a new president
variable using case\_when (6) keep only the variables that Jeff wants us
to keep lol

I will then clean the datasets snp and unemployment in a similar way to
standardize before performing a left merge

``` r
pols_df = read_csv("./data/pols-month.csv") %>%
  janitor::clean_names() %>%
  separate(mon,into = c("year", "month", "day")) %>%
  mutate(
    month = month.abb[as.factor(month)]
  ) %>%
  mutate(
    president = case_when(
      prez_gop == 1 ~ "gop",
      prez_gop == 2 ~ "gop",
      prez_dem == 1 ~ "dem"
    )
  ) %>%
  select(-prez_gop, -prez_dem, -day)
```

    ## Parsed with column specification:
    ## cols(
    ##   mon = col_date(format = ""),
    ##   prez_gop = col_double(),
    ##   gov_gop = col_double(),
    ##   sen_gop = col_double(),
    ##   rep_gop = col_double(),
    ##   prez_dem = col_double(),
    ##   gov_dem = col_double(),
    ##   sen_dem = col_double(),
    ##   rep_dem = col_double()
    ## )

``` r
snp_df = read_csv("./data/snp.csv") %>%
  janitor::clean_names() %>%
  separate(date,into = c("day", "month", "year")) %>%
  mutate(
    month = month.abb[as.factor(month)],
    year = as.factor(year)
  ) %>%
  relocate(year,month)
```

    ## Parsed with column specification:
    ## cols(
    ##   date = col_character(),
    ##   close = col_double()
    ## )

``` r
unemployment_df = read_csv("./data/unemployment.csv") %>%
  mutate(
    Year = as.factor(Year)
  ) %>%
  pivot_longer(
    Jan:Dec,
    names_to = "month",
    values_to = "pct_unemployed"
  ) %>%
  rename(
    year = Year
  ) %>% mutate (
    month = month.abb[as.factor(month)]
  )
```

    ## Parsed with column specification:
    ## cols(
    ##   Year = col_double(),
    ##   Jan = col_double(),
    ##   Feb = col_double(),
    ##   Mar = col_double(),
    ##   Apr = col_double(),
    ##   May = col_double(),
    ##   Jun = col_double(),
    ##   Jul = col_double(),
    ##   Aug = col_double(),
    ##   Sep = col_double(),
    ##   Oct = col_double(),
    ##   Nov = col_double(),
    ##   Dec = col_double()
    ## )

``` r
composite_538 =
  left_join(pols_df, snp_df, by = c("month", "year"))
            
composite_again =
  left_join(composite_538, unemployment_df, by = c("month", "year"))
```

#### About the FiveThirtyEight data

The original datasets, pols-month, snp, and unemployment, came from the
FiveThirtyEight website founded by statistician Nate Silver (source:
p8105 webpage under datasets). Pols-month had information relating to
the “number of national politicians who are democratic or republican at
a given time” (<https://p8105.com/dataset_fivethirtyeight.html>). A fun
snippet of information from this data set is that in 1974, there were 2
republican presidents. This is because 1974 was the year that Nixon was
impeached.

the snp dataset contained information related to “Standard & Poor’s
stock market index (S\&P), often used as a representative measure of
stock market as a whole.” The unemployment dataset contained percentage
unemployment by month and year.

When these three datasets were combined into my composite\_again dataset
(done through two left joins), the result is the following:

there are **1391** observations and **12** variables, which are: **year,
month, gov\_gop, sen\_gop, rep\_gop, gov\_dem, sen\_dem, rep\_dem,
president, day, close, pct\_unemployed**

The dataset includes observations from the years **1947** and **2015**.
The final data is also tidier than the original datasets, as we
collapsed some columns into one column (pivot\_longer on month) as well
as cleaned variable names to follow good naming conventions, and
collapsed two separate prez variables into one president variable.
